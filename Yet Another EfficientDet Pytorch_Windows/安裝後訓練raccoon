# ------------------------------------- Train a custom dataset(raccoon) with pretrained weights--------------------------
# ------------------------------------- 用custom Dataset(raccoon) 再次訓練 -----------------------------------------------                               
# custom Dataset : coco_raccoon 

    env@python3.6, pytorch1.4.0, torchvision0.5.0, cuda10.1, cudnn 7.6.5
    (另外一個環境測試也OK,  pytorch1.2.0, torchvision0.4.0, cuda10.0, cudnn 7.4.2)


[參考1] csv來源 (raccoon), https://github.com/datitran/raccoon_dataset
#下載 https://github.com/datitran/raccoon_dataset
#執行 python xml_to_csv.py  
 生成 raccoon_labels.csv
#將raccoon_labels.csv中 weight欄, height欄刪除 + 將class欄位移到ymax右邊 + 刪除第一列filename-xmin-ymin-xmax-ymax
#另存 raccoon_labels_1090702siquare.csv


[參考2] csv2coco 作法, https://github.com/spytensor/prepare_detection_dataset
#下載 https://github.com/spytensor/prepare_detection_dataset
#新增資料夾 csv
#將raccoon_dataset-master/images 複製到 prepare_detection_dataset-master/csv
#將raccoon_dataset-master/raccoon_labels_1090702siquare.csv 複製到 prepare_detection_dataset-master/csv
#打開csv2coco.py，修改如下並另存 csv2coco_1090702siquare.py
  classname_to_id = {"raccoon": 1}
  csv_file =  "csv/raccoon_labels_1090702siquare.csv"
  image_dir = "csv/images/"   
#執行 python csv2coco_1090702siquare.py
 生成 coco/
          annotations/
                      instances_train2017.json
                      instances_val2017.json
          images/
                train2017/
                          raccoon-1.jpg
                          raccoon-10.jpg
                          ....
                val2017/
                          raccoon-107.jpg
                          raccoon-109.jpg
                          ....

#將prepare_detection_dataset-master/coco資料夾 複製到 Yet-Another-EfficientDet-Pytorch-master/datasets
#將coco資料夾 更名為 coco_raccoon
#將coco_raccoon/images/train2017 移到上一層
#將coco_raccoon/images/val2017 移到上一層
#將coco_raccoon/images已無東西，就刪除


# Win10
可直接執行 python train.py -c 0 -p coco_raccoon --head_only True --lr 1e-3 --batch_size 32 --load_weights weights/efficientdet-d0.pth  --num_epochs 50



# Colab 
可直接執行 train_shape_raccoon_1090702Colab.ipynb


# Colab訓練紀錄
(1) ! python train.py -c 0 -p coco_raccoon --head_only True --lr 1e-3 --batch_size 32 --load_weights weights/efficientdet-d0.pth  --num_epochs 50
Step: 249. Epoch: 49/50. Iteration: 5/5. Cls loss: 0.53440. Reg loss: 0.01692. Total loss: 0.55132: 100% 5/5 [00:05<00:00,  1.15s/it]
Val. Epoch: 49/50. Classification loss: 0.58913. Regression loss: 0.02335. Total loss: 0.61248

(2) ! python train.py -c 0 -p coco_raccoon --head_only True --lr 1e-3 --batch_size 32 --load_weights weights/efficientdet-d0.pth  --num_epochs 200
Step: 999. Epoch: 199/200. Iteration: 5/5. Cls loss: 0.25192. Reg loss: 0.01236. Total loss: 0.26428: 100% 5/5 [00:05<00:00,  1.19s/it]
Val. Epoch: 199/200. Classification loss: 0.21955. Regression loss: 0.02528. Total loss: 0.24483

(3) ! python train.py -c 0 -p coco_raccoon --head_only True --lr 1e-3 --batch_size 32 --load_weights weights/efficientdet-d0.pth  --num_epochs 800
Step: 3999. Epoch: 799/800. Iteration: 5/5. Cls loss: 0.20332. Reg loss: 0.01030. Total loss: 0.21362: 100% 5/5 [00:05<00:00,  1.20s/it]
Val. Epoch: 799/800. Classification loss: 0.20138. Regression loss: 0.02516. Total loss: 0.22653

# Colab訓練紀錄
(1)! python train.py -c 0 -p coco_pets --head_only True --lr 1e-3 --batch_size 128 --load_weights weights/efficientdet-d0.pth  --num_epochs 159
<note>TWCC開4張GPU，但GPU使用率只停在60~90% ; 而且batch size 128讓total loss降很慢
Step: 3679. Epoch: 159/500. Iteration: 23/23. Cls loss: 0.81554. Reg loss: 0.02523. Total loss: 0.84077: 100%|█| 23/23 [00:56<00:00,  1.79s/it]
Val. Epoch: 159/500. Classification loss: 0.81602. Regression loss: 0.02990. Total loss: 0.84592

(2)! python train_GPUsRate.py -c 0 -p coco_pets --head_only True --lr 1e-3 --batch_size 64 --load_weights weights/efficientdet-d0.pth  --num_epochs 200
<note>TWCC開2張GPU，但GPU使用率只停在______% ; 而且batch size 64讓total loss降很慢
Step: 3682. Epoch: 160/500. Iteration: 3/23. Cls loss: 0.80665. Reg loss: 0.02438. Total loss: 0.83103:  13%|▏| 3/23 [00:20<02:48,  8.43s/it]^C

